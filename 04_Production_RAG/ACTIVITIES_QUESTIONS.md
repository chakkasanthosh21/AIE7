**####** Question 1
What is the embedding dimension, given that we're using `text-embedding-3-small`?
**####** Answer 1 
    embedding_dim =  1536

**####** Question 2
    
    **####** Question 2.a
    What if the retriever finds no relevant context? 
    
    **####** Answer 2.a 
    ğŸ§© Approach:
	â€¢	After retrieve(), check if state["context"] is empty or too short.
	â€¢	If empty, return a fallback response like:
    â€œI donâ€™t know. Thereâ€™s no relevant information in the provided context.â€

    **####** question 2.b
    What if the response needs fact-checking?

    **####** Answer 2.b
    Add a fact-checking node after the generate() step in the graph.
    ğŸ§© Approach:
	â€¢	Create a new LLM prompt that takes both the context and the generated response.
	â€¢	Ask something like:
    â€œBased on the provided context, is the answer factually supported? Yes or No.â€
	â€¢	If the answer is No, return a warning:
    â€œâš ï¸ This answer may not be fully supported by the documents retrieved. Please verify independently.â€

